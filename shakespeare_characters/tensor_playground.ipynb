{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bbd343",
   "metadata": {},
   "source": [
    "Closely follows Andrej's Karpathy's [Let's build GPT](https://youtu.be/kCc8FmEb1nY). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9c74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "from functools import partial, reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443f6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/shakespeare.txt\") as f: \n",
    "    text = f.read()\n",
    "\n",
    "print(text[200:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a066ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(' '.join(chars))\n",
    "print(f\"vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947a725",
   "metadata": {},
   "source": [
    "Defines tokenizer encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7ca582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "int_of_char = {ch:i for i, ch in enumerate(chars)}\n",
    "char_of_int = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda text: [int_of_char[ch] for ch in text]\n",
    "decode = lambda toks: reduce(lambda x, y: x + y, \n",
    "                             [char_of_int[i] for i in toks]\n",
    "                            )\n",
    "\n",
    "print(encode(\"hi there!\"))\n",
    "print(decode(encode(\"hi there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a3ce7",
   "metadata": {},
   "source": [
    "Let's wrap the entire dataset in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9201aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849f909",
   "metadata": {},
   "source": [
    "Train/val split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608d7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0bf8f",
   "metadata": {},
   "source": [
    "We use context lengths of 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb81c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "batch_size = 4\n",
    "\n",
    "train_data[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf36408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is 47\n",
      "when input is tensor([18, 47]) target is 56\n",
      "when input is tensor([18, 47, 56]) target is 57\n",
      "when input is tensor([18, 47, 56, 57]) target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52a217",
   "metadata": {},
   "source": [
    "We implement batching, i.e stacking multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02dc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(data, context_length=8, batch_size=4): \n",
    "    idxs = torch.randint(len(data)-context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in idxs])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33dc7c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46587c2c",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f802985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, time, channel\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d688b",
   "metadata": {},
   "source": [
    "Suppose we want the representation of a token to be the *average* of all the tokens that came before it. We can do this with a matmul! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "025bca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowert = torch.tril(torch.ones(T, T))\n",
    "avg = lowert/torch.sum(lowert, 1, keepdim=True)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39bc4628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way of making the same matrix. This is the way that generalizes to self-attention. \n",
    "# can think of matrix entries as affinities or interaction strengths\n",
    "# these affinities are going to become data dependent. \n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(torch.tril(wei)==0, float('-inf'))\n",
    "avg = F.softmax(wei, dim=1)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa81f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old representations (batch element 0):\n",
      " tensor([[-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673],\n",
      "        [ 0.8353, -1.9560],\n",
      "        [-0.8003, -0.5045],\n",
      "        [-1.4267,  0.9059],\n",
      "        [ 0.1446,  0.2280],\n",
      "        [-0.2282, -0.6885],\n",
      "        [ 0.1832,  0.6004]])\n",
      "new representations (batch element 0):\n",
      " tensor([[-0.4970,  0.4658],\n",
      "        [-0.3771, -0.3008],\n",
      "        [ 0.0270, -0.8525],\n",
      "        [-0.1798, -0.7655],\n",
      "        [-0.4292, -0.4312],\n",
      "        [-0.3335, -0.3214],\n",
      "        [-0.3185, -0.3738],\n",
      "        [-0.2558, -0.2520]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"old representations (batch element 0):\\n {x[0]}\")\n",
    "print(f\"new representations (batch element 0):\\n {(avg @ x)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa99dc",
   "metadata": {},
   "source": [
    "Now we implement self-attention, where the affinities are learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b633233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4982, 0.5018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3911, 0.3017, 0.3072, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2242, 0.2592, 0.2620, 0.2546, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1601, 0.2244, 0.2238, 0.2179, 0.1738, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1683, 0.1650, 0.1644, 0.1655, 0.1681, 0.1687, 0.0000, 0.0000],\n",
      "        [0.1415, 0.1440, 0.1454, 0.1431, 0.1409, 0.1418, 0.1434, 0.0000],\n",
      "        [0.1257, 0.1242, 0.1232, 0.1248, 0.1263, 0.1254, 0.1246, 0.1258]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# single self-attention head \n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v= value(x) # (B, T, C)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-.5\n",
    "\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # delete to get encoder\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out= wei @ v \n",
    "print(x.shape)\n",
    "print(wei[0])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d236f3d",
   "metadata": {},
   "source": [
    "You can think about attention as communication in a weighted complete graph\n",
    "\n",
    "This is self-attention. In cross-attention, the queries are produced from some separate source $y$, and the keys and values are produced from $x$. \n",
    "\n",
    "Want to do all you can to prevent attention scores from degenerating to one-hot vectors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
